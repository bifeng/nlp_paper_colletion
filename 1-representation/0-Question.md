1. 为什么word2vec训练要用sgd，不能用adam。为什么negative simpling可以估计分配函数？



1. 为啥word2vec学得的权重，恰好可以作为词表示呢？

   

2. word2vec使用了backpropagation，所以还是应该看作神经网络模型，而不是简单的线性模型？

    

3. 为啥transformer没有去取word embedding？

   

4. 如果把BERT看成一个函数，那它在拟合什么样的函数？

   

5. Bert/GPT can be used in generate task ?

   

6. How to embedding words using generative model ?

   

