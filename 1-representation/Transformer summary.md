refer:<br>[放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)

​	NLP的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。如果你听到我对你说：“你欠我那一千万不用还了”和“我欠你那一千万不用还了”，你听到后分别是什么心情？两者区别了解一下；另外，句子中的长距离特征对于理解语义也非常关键，例子参考上图标红的单词，特征抽取器能否具备长距离特征捕获能力这一点对于解决NLP任务来说也是很关键的。（**句子的长短是一个重要的影响因素**！）

### task

+ 第一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别；
+ 第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可；
+ 第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
+ 第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。

### feature

一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性。

选择一个好的特征抽取器，喂给它大量的训练数据，设定好优化目标（loss function），告诉它你想让它干嘛……..然后你觉得你啥也不用干等结果就行了是吧？那你是我见过的整个宇宙中最乐观的人…….你大量时间其实是用在调参上…….。



### compare

RNN<br>长距离特征: LSTM和GRU的隐层传递机制



CNN<br>长距离特征: Dilated 卷积<br>		    Skip Connection及各种Norm等参数优化技术 + 深度

相对位置信息: 卷积核<br>		        position embedding



transformer<br>长距离特征: Self attention

相对位置信息: position embedding



在特定的长距离特征捕获能力测试任务中（主语-谓语一致性检测，比如we……..are…），实验支持如下结论：原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer>RNN>>CNN; 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。



Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响。

Transformer作为一个新模型，并不是完美无缺的。它也有明显的缺点：首先，对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。所以估计短期内这些领地还能是RNN或者长成Transformer模样的CNN的天下（其实目前他俩这块做得也不好），也是目前看两者的希望所在，尤其是CNN模型，希望更大一些。但是是否Transformer针对长输入就束手无策，没有解决办法呢？我觉得其实并不是，比如拍脑袋一想，就能想到一些方法，虽然看上去有点丑陋。比如可以把长输入切断分成K份，强制把长输入切短，再套上Transformer作为特征抽取器，高层可以用RNN或者另外一层Transformer来接力，形成Transformer的层级结构，这样可以把n平方的计算量极大减少。当然，这个方案不优雅，这个我承认。但是我提示你一下：这个方向是个值得投入精力的好方向，你留意一下我这句话，也许有意想不到的收获。（注：上面这段话是我之前早已写好的，结果今天（1月12日）看见媒体号在炒作：“Transforme-XL，速度提升1800倍”云云。看了新闻，我找来Transformer-XL论文看了一下，发现它解决的就是输入特别长的问题，方法呢其实大思路和上面说的内容差不太多。说这么多的意思是：我并不想删除上面内容，为避免发出来后，那位“爱挑刺”同学说我拷贝别人思路没引用。我决定还是不改上面的说法，因为这个点子实在是太容易想到的点子，我相信你也能想到。）除了这个缺点，Transformer整体结构确实显得复杂了一些，如何更深刻认识它的作用机理，然后进一步简化它，这也是一个好的探索方向，这句话也请留意。还有，上面在做语义特征抽取能力比较时，结论是对于距离远与13的长距离特征，Transformer性能弱于RNN，说实话，这点是比较出乎我意料的，因为Transformer通过Self attention使得远距离特征直接发生关系，按理说距离不应该成为它的问题，但是效果竟然不如RNN，这背后的原因是什么呢？这也是很有价值的一个探索点。



Transformer base和Transformer Big。两者结构其实是一样的，主要区别是包含的Transformer Block数量不同，Transformer base包含12个Block叠加，而Transformer Big则扩张一倍，包含24个Block。无疑Transformer Big在网络深度，参数量以及计算量相对Transformer base翻倍，所以是相对重的一个模型，但是效果也最好。

Transformer XL

