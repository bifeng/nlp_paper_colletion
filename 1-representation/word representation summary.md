

#### PMI-based

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions of the Association for Computational
Linguistics, 4:385–399, 2016.

Arora et al. (2016) propose a generative model (named RAND-WALK) of sentences, where every word is parameterized by a d-dimensional vector. With a key postulate that the word vectors are angularly uniform (“isotropic"), the family of PMI-based word representations can be explained under the RAND-WALK model in terms of the maximum likelihood rule.



Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Interspeech, volume 2, pp. 3, 2010.

Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances
in neural information processing systems, pp. 2177–2185, 2014.

Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In EMNLP, volume 14, pp. 1532–43, 2014.

#### CCA-based

CCA-based spectral factorization approaches

Karl Stratos, Michael Collins, and Daniel Hsu. Model-based word embeddings from decompositions
of count matrices. In Proceedings of ACL, pp. 1282–1291, 2015.

#### paper

+ Sanjeev Arora. Word embeddings: Explaining their properties, 2016. URL http://www.offconvex.org/2016/02/14/word-embeddings-2/. [Online; accessed 16-May-2018].

+ 

  



