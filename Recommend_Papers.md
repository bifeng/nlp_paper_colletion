[100 Must-Read NLP Papers](https://github.com/mhagiwara/100-nlp-papers)



## Segmentation, Tagging, Parsing

* Michael Collins: [Head-Driven Statistical Models for Natural Language Parsing](http://www.cs.columbia.edu/~mcollins/papers/thesis.ps), PhD Dissertation, University of Pennsylvania, 1999.
* Michael Collins: [Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms](http://www.cs.columbia.edu/~mcollins/papers/tagperc.pdf), EMNLP 2002. *(Received Best Paper Award)* 



## Machine Translation & Transliteration, Sequence-to-Sequence Models

+ Ashish Vaswani, et al.: Attention Is All You Need, 2017. 


## Neural Models

- Matthew E. Peters, et al.: Deep contextualized word representations, 2018. [arxiv](https://arxiv.org/abs/1802.05365?context=cs) 
- Jacob Devlin, et al.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018. [arxiv](https://arxiv.org/abs/1810.04805?context=cs) 
- 