<https://github.com/thunlp/PLMpapers>

<https://github.com/daiwk/daiwk.github.io/blob/master/_posts/2019-03-20-nlp-paddle-lark.md>



### Word embedding

#### FastText

<https://github.com/facebookresearch/fastText>

<https://fasttext.cc/docs/en/autotune.html>







### Transformer-based



#### Transformers

🤗 Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. https://huggingface.co/transformers

https://github.com/huggingface/transformers



#### RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov [arxiv](<https://arxiv.org/abs/1907.11692>) 



#### XLNet

https://arxiv.org/abs/1906.08237

https://github.com/zihangdai/xlnet



#### ALBERT

<https://github.com/google-research/ALBERT>

Notes on ALBERT” by Rémi Louf paper:《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》 



#### ELECTRA

ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ICLR 2020

<https://zhuanlan.zhihu.com/p/89763176>

不仅提出了NLP式的Generator-Discriminator，而且文章简直就是最近nlp发展的timeline。顺着文章读下去，然后跟进参考文献，基本就可以把最近nlp的发展理得一清二楚。并且，也简单总结了目前各模型存在的缺点。 Manning PPT链接在下面。https://www.jianguoyun.com/p/DVRRLHUQq7ftBxjG3Y8C 

链接:[https://pan.baidu.com/s/172hClbA4fwON9MFzspnRBw](https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/172hClbA4fwON9MFzspnRBw) 密码:b6qg

第一个有效学到用当前位置提取的信息预测当前位置的输出的模型，之前的bert/xlnet等都是用别的位置提取的信息预测当前位置的输出.



