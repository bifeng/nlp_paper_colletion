<https://github.com/thunlp/PLMpapers>

<https://github.com/daiwk/daiwk.github.io/blob/master/_posts/2019-03-20-nlp-paddle-lark.md>



### Word embedding

#### FastText

<https://github.com/facebookresearch/fastText>

<https://fasttext.cc/docs/en/autotune.html>







### Transformer-based



#### Transformers

ğŸ¤— Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. https://huggingface.co/transformers

https://github.com/huggingface/transformers



#### RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov [arxiv](<https://arxiv.org/abs/1907.11692>) 



#### XLNet

https://arxiv.org/abs/1906.08237

https://github.com/zihangdai/xlnet



#### ALBERT

<https://github.com/google-research/ALBERT>



#### ELECTRA

ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ICLR 2020

<https://zhuanlan.zhihu.com/p/89763176>

ä¸ä»…æå‡ºäº†NLPå¼çš„Generator-Discriminatorï¼Œè€Œä¸”æ–‡ç« ç®€ç›´å°±æ˜¯æœ€è¿‘nlpå‘å±•çš„timelineã€‚é¡ºç€æ–‡ç« è¯»ä¸‹å»ï¼Œç„¶åè·Ÿè¿›å‚è€ƒæ–‡çŒ®ï¼ŒåŸºæœ¬å°±å¯ä»¥æŠŠæœ€è¿‘nlpçš„å‘å±•ç†å¾—ä¸€æ¸…äºŒæ¥šã€‚å¹¶ä¸”ï¼Œä¹Ÿç®€å•æ€»ç»“äº†ç›®å‰å„æ¨¡å‹å­˜åœ¨çš„ç¼ºç‚¹ã€‚ Manning PPTé“¾æ¥åœ¨ä¸‹é¢ã€‚https://www.jianguoyun.com/p/DVRRLHUQq7ftBxjG3Y8C 

é“¾æ¥:[https://pan.baidu.com/s/172hClbA4fwON9MFzspnRBw](https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/172hClbA4fwON9MFzspnRBw) å¯†ç :b6qg

ç¬¬ä¸€ä¸ªæœ‰æ•ˆå­¦åˆ°ç”¨å½“å‰ä½ç½®æå–çš„ä¿¡æ¯é¢„æµ‹å½“å‰ä½ç½®çš„è¾“å‡ºçš„æ¨¡å‹ï¼Œä¹‹å‰çš„bert/xlnetç­‰éƒ½æ˜¯ç”¨åˆ«çš„ä½ç½®æå–çš„ä¿¡æ¯é¢„æµ‹å½“å‰ä½ç½®çš„è¾“å‡º.



