<https://github.com/thunlp/PLMpapers>

<https://github.com/daiwk/daiwk.github.io/blob/master/_posts/2019-03-20-nlp-paddle-lark.md>



## Word embedding

#### FastText

<https://github.com/facebookresearch/fastText>

<https://fasttext.cc/docs/en/autotune.html>







## Transformer-based



#### Transformers

ğŸ¤— Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. https://huggingface.co/transformers

https://github.com/huggingface/transformers



#### RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov [arxiv](<https://arxiv.org/abs/1907.11692>) 



#### XLNet

https://arxiv.org/abs/1906.08237

https://github.com/zihangdai/xlnet



### ALBERT

ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. [arxiv](<https://arxiv.org/abs/1909.11942>) 

<https://github.com/google-research/ALBERT>

Notes on ALBERTâ€ by RÃ©mi Louf paper:ã€ŠALBERT: A Lite BERT for Self-supervised Learning of Language Representationsã€‹ 

**two parameter reduction techniques**

+ Factorized embedding parameterization

  The WordPiece embedding size E is tied with the hidden layer size H, i.e., E â‰¡ H. This decision appears suboptimal for both modeling and practical reasons, as follows.

  From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations.

  From a practical perspective, natural language processing usually require the vocabulary size V to be large. If E â‰¡ H, then increasing H increases the size of the embedding matrix, which has size V Ã—E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training.

  we use a factorization of the embedding parameters, decomposing them
  into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space.

  <u>We choose to use the same E for all word pieces because they are much more evenly distributed across documents compared to whole-word embedding, where having different embedding size (Grave et al. (2017); Baevski & Auli (2018); Dai et al. (2019) ) for different words is important</u>.(???)

+ cross-layer parameter sharing

  There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters.
  The default decision for ALBERT is to share all parameters across layers.





**a self-supervised loss** - Inter-sentence coherence loss

a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. 



#### ELECTRA

ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ICLR 2020

<https://zhuanlan.zhihu.com/p/89763176>

ä¸ä»…æå‡ºäº†NLPå¼çš„Generator-Discriminatorï¼Œè€Œä¸”æ–‡ç« ç®€ç›´å°±æ˜¯æœ€è¿‘nlpå‘å±•çš„timelineã€‚é¡ºç€æ–‡ç« è¯»ä¸‹å»ï¼Œç„¶åè·Ÿè¿›å‚è€ƒæ–‡çŒ®ï¼ŒåŸºæœ¬å°±å¯ä»¥æŠŠæœ€è¿‘nlpçš„å‘å±•ç†å¾—ä¸€æ¸…äºŒæ¥šã€‚å¹¶ä¸”ï¼Œä¹Ÿç®€å•æ€»ç»“äº†ç›®å‰å„æ¨¡å‹å­˜åœ¨çš„ç¼ºç‚¹ã€‚ Manning PPTé“¾æ¥åœ¨ä¸‹é¢ã€‚https://www.jianguoyun.com/p/DVRRLHUQq7ftBxjG3Y8C 

é“¾æ¥:[https://pan.baidu.com/s/172hClbA4fwON9MFzspnRBw](https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/172hClbA4fwON9MFzspnRBw) å¯†ç :b6qg

ç¬¬ä¸€ä¸ªæœ‰æ•ˆå­¦åˆ°ç”¨å½“å‰ä½ç½®æå–çš„ä¿¡æ¯é¢„æµ‹å½“å‰ä½ç½®çš„è¾“å‡ºçš„æ¨¡å‹ï¼Œä¹‹å‰çš„bert/xlnetç­‰éƒ½æ˜¯ç”¨åˆ«çš„ä½ç½®æå–çš„ä¿¡æ¯é¢„æµ‹å½“å‰ä½ç½®çš„è¾“å‡º.



