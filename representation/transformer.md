<http://jalammar.github.io/illustrated-transformer/>

<https://nlp.seas.harvard.edu/2018/04/03/attention.html>



### Question

+ how to select proper parameters for transformer ?




### Case

+ transformer case [code](<https://github.com/tensorflow/tensor2tensor>) :star::star::star::star::star:
+ sentiment analysis [site](<https://mc.ai/deep-learning-in-production-sentiment-analysis-with-the-transformer-model/>) [code](<https://github.com/cortexlabs/cortex/blob/master/examples/pipelines/reviews/implementations/models/transformer.py>) 

### Slides

+ 序列到序列模型 李航 [slides](https://originalstatic.aminer.cn/misc/billboard/aml/20200408-李航-Sequence to Sequence Models(1).pdf)  



### Literature Review

+ Efficient Transformers: A Survey. Y Tay, M Dehghani, D Bahri, D Metzler [Google Research] (2020) 
+ <https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html>



### Paper

+ Transformer Details Not Described in The Paper <https://tunz.kr/post/4>

+ Augmenting Self-attention with Persistent Memory

+ Adaptive Attention Span for Transformers, Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin, ACL 2019 [arxiv](<https://arxiv.org/abs/1905.07799>) [code](<https://github.com/facebookresearch/adaptive-span>) 

+ sparse transformer

  Generating Long Sequences with Sparse Transformers, Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, [arxiv](<https://arxiv.org/abs/1904.10509>) [code](<https://github.com/openai/sparse_attention>) 

+ Training Tips for the Transformer Model, Martin Popel, Ondřej Bojar, PBML [arxiv](<https://arxiv.org/abs/1804.00247>) 
